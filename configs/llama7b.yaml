model_path: /home/cl/stanford_alpaca/llama7Bhf/llama-7b # hugging face format model dir, loaded through AutoModel
model_name: llama # just a name for log file
batch_size: 20

zero_shot_template: 
  no_input: "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:"
  with_input: "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:"

few_shot_template:
  instance_template: "{input} {output}"
  split_by: ","
  question_prompt: " {input}"

generate_config:
  do_sample: False
  temperature: 1.0
  top_p: 1.0
  top_k: 10
  num_beams: 1
  max_new_tokens: 5
